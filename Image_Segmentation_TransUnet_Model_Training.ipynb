{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHjfU5SuqAnO"
      },
      "source": [
        "## EMATM0047: Data Science Project\n",
        "---\n",
        "### Code Section S2.2: Image segmentation - TransUnet model training section\n",
        "#### Author: Alan Liu\n",
        "#### Faculty of Engineering\n",
        "#### University of Bristol\n",
        "\n",
        "Input:\n",
        "1. the cryo-EM picture, 4 stages with 100 .mrc each, 400 files in total.\n",
        "2. the corresponding mask, 4 stages with 100 .npy each, 400 files in total.\n",
        "\n",
        "Operation:\n",
        "1. Construct the DataLoader\n",
        "2. Constructure TransUnet model for 7 stages\n",
        "3. Conduct the model training, followed by a particle picking section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jms4trrVrBAf"
      },
      "source": [
        "Connect to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id9XFq1Vniuv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9OExA8Rs1n0"
      },
      "source": [
        "Validate if the mrcfile is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWjNF836rKTz"
      },
      "outputs": [],
      "source": [
        "# check if mrcfile is installed\n",
        "# check if ml-collections is installed, to read the configDict\n",
        "try:\n",
        "  import mrcfile\n",
        "  print(\"Yes\")\n",
        "except:\n",
        "  print(\"No\")\n",
        "  !pip install mrcfile\n",
        "  !pip install ml-collections\n",
        "  print()\n",
        "  print(\"mrcfile and ml-connections installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm1j9RvQHNHp"
      },
      "source": [
        "Here we follow the README file from the TransUnet github repository to finish all preparation works.\n",
        "\n",
        "[The TransUnet repo](https://github.com/Beckschen/TransUNet)\n",
        "\n",
        "### Step 1: clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB6p0bJOukSh"
      },
      "outputs": [],
      "source": [
        "# make a new folder\n",
        "!mkdir -p \"/content/drive/MyDrive/Final Project/TransUnet\"\n",
        "# switch to this path\n",
        "%cd \"/content/drive/MyDrive/Final Project/TransUnet\"\n",
        "# clone the repo\n",
        "!git clone https://github.com/Beckschen/TransUNet.git\n",
        "\n",
        "# The code only need to be run once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5cDTV7pOwZr"
      },
      "source": [
        "### Step 2: install the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFwbMzPMPLlh"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/Final Project/TransUnet\"\n",
        "%cd TransUNet\n",
        "# install the dependencies\n",
        "# original file requires torch==1.4.0 but unavailable here\n",
        "!pip install -r requirements.txt\n",
        "# thus, try import torch and view its version\n",
        "try:\n",
        "  import torch\n",
        "  print(\"Torch installed with version:\", torch.__version__)\n",
        "except:\n",
        "  raise Exception(\"Torch not installed\")\n",
        "\n",
        "# must run every time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQk_NIZUE7p"
      },
      "source": [
        "### Step 3: Download the pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL2w46KlUiIE"
      },
      "outputs": [],
      "source": [
        "# here we use the R50+ViT-B_16\n",
        "# download\n",
        "!wget https://storage.googleapis.com/vit_models/imagenet21k/R50%2BViT-B_16.npz\n",
        "# create new dict\n",
        "try:\n",
        "    !mkdir -p \"/content/drive/MyDrive/Final Project/TransUnet/model/vit_checkpoint/imagenet21k\"\n",
        "    print(\"Folder created\")\n",
        "    # move the weight\n",
        "    !mv R50+ViT-B_16.npz \"/content/drive/MyDrive/Final Project/TransUnet/model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz\"\n",
        "    print(\"Weight moved\")\n",
        "except:\n",
        "    raise Exception(\"Encounter error\")\n",
        "\n",
        "# The code only need to be run once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX36zxg3jh5P"
      },
      "source": [
        "### Step 4: Construct the dataloader\n",
        "The dataloader can be copied from the Unet part, with a little modifation\n",
        "\n",
        "1. set the patch_size to 224 specifically, default in TransUnet\n",
        "2. stack up the mrc to 3 channels, RGB input is mandatory for transUnet\n",
        "3. for the noramlization method, use the ImageNet normalization with:\n",
        "\n",
        " mean: [0.485, 0.456, 0.406]\n",
        "\n",
        " std: [0.229, 0.224, 0.225]\n",
        "4. change the shape of mrc from the (H, W, C) to (C, H, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Za4nTBDje1k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import mrcfile\n",
        "from pathlib import Path\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "# set the random seed to make sure the result can be reproduced\n",
        "seed = 426 # according to my ID\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "class PatchBasedData(Dataset):\n",
        "    \"\"\"\n",
        "    process the original dataset with the patch_based approach\n",
        "\n",
        "    the process steps are:\n",
        "    1. Find all MRC and mask files\n",
        "    1. For file with cells: generate patches around the cells\n",
        "    2. For file without cells: generate patches randomly\n",
        "    3. all patches are restricted with the same size of 256 * 256\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str = \"/content/drive/MyDrive/Final Project\",\n",
        "                 stages: Optional[List[int]] = None, # which stage to process\n",
        "                 patch_size: int = 224, # size of patch, Note; this number must be lower than h, w of mask\n",
        "                 patches_per_image: int = 24, # how many patches are needed for one mrc file\n",
        "                 bg_patches_per_image: int = 12): # how many are needed, for no-cell mrc file\n",
        "\n",
        "        self.base_dir = Path(base_dir)\n",
        "        assert patch_size == 224 # patch_size must be 224\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_per_image = patches_per_image\n",
        "        self.bg_patches_per_image = bg_patches_per_image\n",
        "\n",
        "        # stage mapping\n",
        "        # because the name of stage folder of mrc and npy are different\n",
        "        self.stage_mapping = {1: {'mrc': 'stageI', 'mask': 'stage1'},\n",
        "                    2: {'mrc': 'stageII', 'mask': 'stage2'},\n",
        "                    3: {'mrc': 'stageIII', 'mask': 'stage3'},\n",
        "                    4: {'mrc': 'stageIV', 'mask': 'stage4'}}\n",
        "\n",
        "        # all 4 stages will be processed as default\n",
        "        if stages is None:\n",
        "            stages = [1, 2, 3, 4]\n",
        "        self.stages = stages\n",
        "\n",
        "        # load all files\n",
        "        self.data_pairs = []\n",
        "        self.load_file_paths()\n",
        "\n",
        "        # pre-load the data\n",
        "        # shorten the processing time\n",
        "        print(\"Preloading data into memory\")\n",
        "        self.preload_data()\n",
        "\n",
        "        # generate all patches\n",
        "        self.patches = []\n",
        "        self.generate_all_patches()\n",
        "\n",
        "    def load_file_paths(self):\n",
        "        \"\"\"\n",
        "        load the mrc and mask file\n",
        "        save them as pairs\n",
        "        \"\"\"\n",
        "        # note: this path is exclusive to my computer\n",
        "        # it may be changed in different devices\n",
        "        mrc_base = self.base_dir / \"Dataset-processed\"\n",
        "        mask_base = self.base_dir / \"Image-segmentation-Level-4\"\n",
        "\n",
        "        # get the stage folder\n",
        "        for stage in self.stages:\n",
        "            mrc_dir = mrc_base / self.stage_mapping[stage]['mrc']\n",
        "            mask_dir = mask_base / self.stage_mapping[stage]['mask']\n",
        "\n",
        "            # get all mrc files\n",
        "            mrc_files = sorted([f for f in os.listdir(mrc_dir) if f.endswith('.mrc')])\n",
        "\n",
        "            # get the npy on the basis of each mrc\n",
        "            for mrc_file in mrc_files:\n",
        "                # the path of one mrc\n",
        "                mrc_path = mrc_dir / mrc_file\n",
        "                # construct the name of mask with mrc\n",
        "                mask_file = f\"mask_{mrc_file.split('.')[0]}_v4.npy\"\n",
        "                # the path of the mask\n",
        "                mask_path = mask_dir / mask_file\n",
        "\n",
        "                # if pair is found, save them with the stage label\n",
        "                if mrc_path.exists() and mask_path.exists():\n",
        "                    self.data_pairs.append({'mrc_path': mrc_path,\n",
        "                                'mask_path': mask_path,\n",
        "                                'stage': stage})\n",
        "                else:\n",
        "                    print(f\"Missing file: {mrc_path} or {mask_path}\")\n",
        "                    continue\n",
        "\n",
        "        # the correct/ideal number: len(self.stages) * 100\n",
        "        print(f\"{len(self.data_pairs)} pairs in total\")\n",
        "\n",
        "    def preload_data(self):\n",
        "        \"\"\"\n",
        "        pre-load the data into memory\n",
        "        \"\"\"\n",
        "        self.cache_mrc = {}\n",
        "        self.cache_mask = {}\n",
        "        # load and cache the mrc and mask\n",
        "        for idx, pair in enumerate(tqdm(self.data_pairs, desc = 'Loading data')):\n",
        "            # load mrc\n",
        "            mrc_data = self.load_mrc(pair['mrc_path'])\n",
        "            # normalize mrc\n",
        "            mrc_data = self.normalize_mrc(mrc_data)\n",
        "            # load mask\n",
        "            mask_data = np.load(pair['mask_path'])\n",
        "\n",
        "            # cache them\n",
        "            self.cache_mrc[idx] = mrc_data\n",
        "            self.cache_mask[idx] = mask_data\n",
        "\n",
        "\n",
        "    def generate_all_patches(self):\n",
        "        \"\"\"\n",
        "        generate the patches for all files\n",
        "        \"\"\"\n",
        "        print(\"Generating patches...\")\n",
        "\n",
        "        # number of no cell mask\n",
        "        no_cell_images = 0\n",
        "\n",
        "        for idx, pair in enumerate(tqdm(self.data_pairs, desc = 'generating patches')):\n",
        "            # load the npy\n",
        "            mask = np.load(pair['mask_path'])\n",
        "            # the corresponding stage label\n",
        "            stage = pair['stage']\n",
        "\n",
        "            # find all positions of the cell and return (rows, columns)\n",
        "            cell_positions = np.where(mask == stage)\n",
        "\n",
        "            if len(cell_positions[0]) > 0:\n",
        "                # cell patch\n",
        "                self.generate_cell_patches(idx, mask, stage)\n",
        "            else:\n",
        "                # no cell patch\n",
        "                no_cell_images += 1\n",
        "                self.generate_background_patches(idx, mask, stage)\n",
        "\n",
        "        print(f\"Found {no_cell_images} images without cells\")\n",
        "        print(f\"Generated {len(self.patches)} total patches\")\n",
        "\n",
        "    def generate_cell_patches(self, pair_idx, mask, stage):\n",
        "        \"\"\"\n",
        "        generate the patches for mrc that have cells\n",
        "\n",
        "        argument:\n",
        "        pair_idx: the index of the pair\n",
        "        mask: the Numpy array mask\n",
        "        stage: the stage label\n",
        "        \"\"\"\n",
        "        # get the mask shape\n",
        "        h, w = mask.shape\n",
        "\n",
        "        # define the stride, half of the patch size\n",
        "        stride = self.patch_size // 2\n",
        "\n",
        "        # the list storing the x,y and ratio\n",
        "        patch_in_single = []\n",
        "\n",
        "        # get all patch\n",
        "        for y in range(0, h - self.patch_size + 1, stride):\n",
        "            for x in range(0, w - self.patch_size + 1, stride):\n",
        "\n",
        "              patch = mask[y:y + self.patch_size, x:x + self.patch_size]\n",
        "\n",
        "              # valid pixel only\n",
        "              all_valid = (patch != -1)\n",
        "\n",
        "              # there could have all -1 patch, in actual running\n",
        "              valid_sum = int(np.count_nonzero(all_valid))\n",
        "              if valid_sum == 0:\n",
        "                continue\n",
        "\n",
        "              # get the foregraound ratio: (patch == stage) / all_valid\n",
        "              foreground_pixel = int(((patch == stage) & all_valid).sum())\n",
        "              ratio = float(foreground_pixel) / float(all_valid.sum())\n",
        "\n",
        "              # store the info\n",
        "              patch_in_single.append((x, y, ratio))\n",
        "\n",
        "        # here we use top-K to select patch\n",
        "        # 24 from the top\n",
        "        k_need = self.patches_per_image\n",
        "\n",
        "        # sorting based on ratio\n",
        "        patch_in_single.sort(key = lambda t: t[2], reverse = True)\n",
        "        # topk\n",
        "        top_K = patch_in_single[:k_need]\n",
        "\n",
        "\n",
        "        # record the info\n",
        "        for (x, y, ratio) in top_K:\n",
        "          self.patches.append({'image_idx': pair_idx,\n",
        "                    'y': y,\n",
        "                    'x': x,\n",
        "                    'stage': stage,\n",
        "                    'has_cells': True})\n",
        "\n",
        "    def generate_background_patches(self, pair_idx, mask, stage):\n",
        "        \"\"\"\n",
        "        generate patches for mrc that have no cells\n",
        "\n",
        "        argument:\n",
        "        pair_idx: the index of the pair\n",
        "        mask: the Numpy array mask\n",
        "        stage: the stage label\n",
        "        \"\"\"\n",
        "        # get the mask size\n",
        "        h, w = mask.shape\n",
        "\n",
        "        # get 10 patches randomly\n",
        "        for _ in range(self.bg_patches_per_image):\n",
        "            # make sure inside the mask\n",
        "            y = random.randint(0, h - self.patch_size)\n",
        "            x = random.randint(0, w - self.patch_size)\n",
        "\n",
        "            # save the patch\n",
        "            self.patches.append({'image_idx': pair_idx,\n",
        "                      'y': y,\n",
        "                      'x': x,\n",
        "                      'stage': stage,\n",
        "                      'has_cells': False})\n",
        "\n",
        "    def load_mrc(self, path: Path) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        load the mrc file\n",
        "\n",
        "        argument:\n",
        "        path: the path of the mrc file\n",
        "        \"\"\"\n",
        "        with mrcfile.open(path, mode = 'r') as mrc:\n",
        "            data = mrc.data.copy()\n",
        "        return data\n",
        "\n",
        "    def normalize_mrc(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        normalize the mrc data following the '_load_mrc()'\n",
        "\n",
        "        argument:\n",
        "        data: the mrc data to be normalized\n",
        "        \"\"\"\n",
        "        # use the quantile normalization\n",
        "        # get the 1st percentile and 99th percentile\n",
        "        p1, p99 = np.percentile(data, [1, 99])\n",
        "        # clip the data\n",
        "        data = np.clip(data, p1, p99)\n",
        "        # then map the data to [0, 1]\n",
        "        data = (data - p1) / (p99 - p1 + 1e-8)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        # how many patches in total\n",
        "        return len(self.patches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get the tuple contains ('image_idx','y','x','stage','has_cell')\n",
        "        patch_info = self.patches[idx]\n",
        "        # get the index of the pair\n",
        "        image_idx = patch_info['image_idx']\n",
        "\n",
        "        # load the data from cache\n",
        "        mrc_data = self.cache_mrc[image_idx]\n",
        "        mask_data = self.cache_mask[image_idx]\n",
        "\n",
        "        # get the coordinates\n",
        "        y = patch_info['y']\n",
        "        x = patch_info['x']\n",
        "        # clip the mrc\n",
        "        patch_mrc = mrc_data[y:y + self.patch_size, x:x + self.patch_size]\n",
        "        # clip the mask\n",
        "        patch_mask = mask_data[y:y + self.patch_size, x:x + self.patch_size]\n",
        "\n",
        "        # New part\n",
        "        # preparation: the mrc must be float32 to keep accuracy\n",
        "        patch_mrc = patch_mrc.astype(np.float32)\n",
        "\n",
        "        # step 1: stack up to three channels\n",
        "        if patch_mrc.ndim == 2:\n",
        "            patch_mrc = np.stack([patch_mrc] * 3, axis = 2) # (H, W, 3)\n",
        "\n",
        "        # step 2: use Imagenet normalization\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        patch_mrc = (patch_mrc - mean) / std\n",
        "\n",
        "        # step 3: change the (H, W, 3) to (3, H, W)\n",
        "        patch_mrc = patch_mrc.transpose(2, 0, 1)\n",
        "\n",
        "        # the background of stage 4 is different from others\n",
        "        # thus, we allocate a new label to it\n",
        "        if patch_info['stage'] == 4:\n",
        "          patch_mask[patch_mask == 0] = 5\n",
        "\n",
        "        # for \"-1\" label, map it to 255\n",
        "        patch_mask[patch_mask == -1] = 255\n",
        "\n",
        "        # shift to tensor\n",
        "        # float for mrc, Conv2d\n",
        "        patch_mrc = torch.from_numpy(patch_mrc).float()\n",
        "        # long for mask, calculate loss\n",
        "        patch_mask = torch.from_numpy(patch_mask).long()\n",
        "\n",
        "        return {'image': patch_mrc,\n",
        "            'mask': patch_mask,\n",
        "            'stage': patch_info['stage'],\n",
        "            'has_cells': patch_info['has_cells']}\n",
        "\n",
        "def create_dataloaders(base_dir: str = \"/content/drive/MyDrive/Final Project\",\n",
        "            stages: Optional[List[int]] = None,\n",
        "            batch_size: int = 32,\n",
        "            patch_size: int = 256,\n",
        "            num_workers: int = 8,\n",
        "            val_split: float = 0.2,\n",
        "            min_cell_pixels: int = 200):\n",
        "    \"\"\"\n",
        "    create dataloaders for training and validation\n",
        "\n",
        "    argument:\n",
        "    base_dir: the root directory of the dataset\n",
        "    stages: which stages to process\n",
        "    batch_size: hwo many patches in each batch\n",
        "    patch_size: the size of each patch\n",
        "    num_workers: Number of subprocesses, 0 in windows and changeable in Colab\n",
        "    val_split: the percentage of the validation set\n",
        "    \"\"\"\n",
        "    # get the patch_based dataset\n",
        "    dataset = PatchBasedData(base_dir = base_dir, stages = stages, patch_size = patch_size)\n",
        "\n",
        "    # get the size of training and validation dataset\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(total_size * val_split)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    # split the dataset\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # construct the training dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers,\n",
        "                  pin_memory = True)\n",
        "    # construct the validation dataloader\n",
        "    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers = num_workers,\n",
        "                 pin_memory = True)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def calculate_batch_statistics(batch):\n",
        "    \"\"\"\n",
        "    calculate the statistics of a batch\n",
        "\n",
        "    argument: batch: the batch created by dataloader\n",
        "    \"\"\"\n",
        "    # get all mask patches in a batch\n",
        "    masks = batch['mask']\n",
        "    # dict to store\n",
        "    stats = defaultdict(int)\n",
        "\n",
        "    for mask in masks:\n",
        "        # get each label and their counts\n",
        "        label, counts = torch.unique(mask, return_counts = True)\n",
        "        for label, counts in zip(label.tolist(), counts.tolist()):\n",
        "            # record them\n",
        "            stats[label] += counts\n",
        "    # the total pixels\n",
        "    total = sum(stats.values())\n",
        "    # the valid pixels, by excluding the 255\n",
        "    valid = total - stats.get(255, 0)\n",
        "    # the cell pixels\n",
        "    # [1,2,3,4] are cells\n",
        "    cell_pixels = sum(v for k, v in stats.items() if k in [1, 2, 3, 4])\n",
        "    # summary and return\n",
        "    return {'total_pixels': total,\n",
        "        'valid_pixels': valid,\n",
        "        'cell_ratio': cell_pixels / valid,\n",
        "        'patches_with_cells': sum(batch['has_cells'])}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVNT_rHyNoiq"
      },
      "source": [
        "Next, construct the TransUnet model\n",
        "\n",
        "The origianl repository provides the configuration for constructing the model, which should be modifed based on the actual dataset. The items that must be changed are listed below:\n",
        "\n",
        "1. n_classes: changed to 7, for 4 cells and 2 backgrounds and 1 black border\n",
        "2. pretrained_path: the path where the pretrained weight is stored. Here I saved the weight R50+ViT-B_16.npz\n",
        "3. patches.grid: the ViT segments the input image to patches. For a 224*224 input, it will be divided into 16 x 16 patches, so the size of each patch is 14 x 14. The size of grid is determined by the size of input and how many patches we want.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAsCfSV24o5f"
      },
      "outputs": [],
      "source": [
        "# get the configuration from the offical\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# add the path of the model\n",
        "sys.path.append('/content/drive/MyDrive/Final Project/TransUnet/TransUnet')\n",
        "\n",
        "from networks.vit_seg_configs import get_r50_b16_config\n",
        "from networks.vit_seg_modeling import VisionTransformer\n",
        "\n",
        "def get_vit_config_customed():\n",
        "  # get the offical version\n",
        "  official_config = get_r50_b16_config()\n",
        "  # change the n_classes\n",
        "  official_config.n_classes = 7\n",
        "  # change the weights path\n",
        "  official_config.pretrained_path = '/content/drive/MyDrive/Final Project/TransUnet/model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz'\n",
        "  # change the patch size\n",
        "  official_config.patches.grid = (14, 14)\n",
        "\n",
        "  return official_config\n",
        "\n",
        "def transunet(img_size = (224, 224)):\n",
        "  \"\"\"\n",
        "  construct the TransUnet model\n",
        "\n",
        "  argument:\n",
        "  img_size: the size of the input image\n",
        "  \"\"\"\n",
        "  custom_config = get_vit_config_customed()\n",
        "  # make sure the patch size is right\n",
        "  grid_h = img_size[0] // 16\n",
        "  grid_w = img_size[1] // 16\n",
        "  custom_config.patches.grid = (grid_h, grid_w)\n",
        "\n",
        "  # the name is VisionTransformer\n",
        "  model = VisionTransformer(custom_config, img_size = img_size, num_classes = custom_config.n_classes)\n",
        "\n",
        "  # load weights\n",
        "  weights = np.load(custom_config.pretrained_path, allow_pickle = True)\n",
        "  model.load_from(weights)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5bp8bPe6YiM"
      },
      "outputs": [],
      "source": [
        "# define the loss function, use dice loss + crossentropy again\n",
        "# CrossEntropy: used to ignore the 255 label\n",
        "# Dice loss: can tackle the category imbalance problem\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    calculate the dice loss\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth = 1.0, ignore_index = 255):\n",
        "        super().__init__()\n",
        "        # smoothing factor to prevent zero denominator\n",
        "        self.smooth = smooth\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        # get the probability of each label\n",
        "        # logits: [batch_size, 7 labels, 224, 224]\n",
        "        all_probs = F.softmax(input, dim = 1)\n",
        "\n",
        "        # ignore 255\n",
        "        valid_mask = target != self.ignore_index\n",
        "\n",
        "        # sum the loss for each label\n",
        "        total_loss = 0\n",
        "        classes = input.shape[1]\n",
        "\n",
        "        # in each class\n",
        "        for one_class in range(classes):\n",
        "\n",
        "            # get its probability\n",
        "            prob = all_probs[:, one_class]\n",
        "\n",
        "            # shift the ground truth to 1/0\n",
        "            real = (target == one_class).float()\n",
        "\n",
        "            # in valid zone only\n",
        "            prob = prob[valid_mask]\n",
        "            real = real[valid_mask]\n",
        "\n",
        "            # calculate the dice coefficeint\n",
        "            # calculate dice coefficient: (2 * inter * smooth) / (sum(prob) + sum(real) + smooth)\n",
        "            # inter: the probability on right label\n",
        "            intersection = (prob * real).sum()\n",
        "            dice = (2. * intersection + self.smooth) / (prob.sum() + real.sum() + self.smooth)\n",
        "\n",
        "            # sum to total loss\n",
        "            total_loss += (1 - dice)\n",
        "\n",
        "        return total_loss / classes\n",
        "\n",
        "# combine the dice loss to CrossEntropy\n",
        "class DiceCELoss(nn.Module):\n",
        "  \"\"\"\n",
        "  conbine the Dice loss and CrossEntropy loss\n",
        "  \"\"\"\n",
        "  def __init__(self, dice_weight = 0.7, ce_weight = 0.3):\n",
        "    \"\"\"\n",
        "    argument:\n",
        "    dice_weight: the weight of Dice loss\n",
        "    ce_weight: the weight of CrossEntropy loss\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.dice_weight = dice_weight\n",
        "    self.ce_weight = ce_weight\n",
        "\n",
        "    # the crossentropy with ignore index\n",
        "    self.ce = nn.CrossEntropyLoss(ignore_index = 255)\n",
        "    # set up the dice loss\n",
        "    self.dice = DiceLoss()\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    # calculate crossentropy Loss\n",
        "    ce_loss = self.ce(input, target)\n",
        "\n",
        "    # calculate dice loss\n",
        "    dice_loss = self.dice(input, target)\n",
        "\n",
        "    # combine them\n",
        "    total_loss = self.ce_weight * ce_loss + self.dice_weight * dice_loss\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCAgzc1p8CW8"
      },
      "outputs": [],
      "source": [
        "# training part\n",
        "def train_model(model, train_loader, val_loader, epochs = 30, dice_weight = 0.6, ce_weight = 0.4, lr = 1e-3,\n",
        "        device = 'cuda', save_path  = \"TransUnet_seg_weights.pth\"):\n",
        "    \"\"\"\n",
        "    training section\n",
        "\n",
        "    argument:\n",
        "    model: the Unet model\n",
        "    train_loader: the training dataloader\n",
        "    val_loader: the validation dataloader\n",
        "    epochs: how many epochs to train\n",
        "    dice_weight: the weight of Dice loss\n",
        "    ce_weight: the weight of CrossEntropy loss\n",
        "    device: where to train the model, 'cuda' or 'cpu'\n",
        "    \"\"\"\n",
        "    # move the model to cuda\n",
        "    model = model.to(device)\n",
        "    # loss function\n",
        "    criterion = DiceCELoss(dice_weight = dice_weight, ce_weight = ce_weight)\n",
        "    # for the optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr = lr, weight_decay = 1e-4)\n",
        "    # for the scheduler, they use the cosine annealing\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs, eta_min = 0.0)\n",
        "    # best val loss\n",
        "    best_val_iou = 0\n",
        "\n",
        "    # 1,2,3,4 only\n",
        "    particle_class = [1,2,3,4]\n",
        "\n",
        "    # include 0, the background\n",
        "    all_class = [0,1,2,3,4]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        # the loss of the training set\n",
        "        train_loss = 0.0\n",
        "        # calculate the IOU\n",
        "        train_iou = {stage: {'inter': 0, 'union': 0} for stage in particle_class}\n",
        "\n",
        "        # use the tqdm to show the progress bar\n",
        "        pbar = tqdm(train_loader, desc = f'Epoch {epoch + 1} / {epochs} [Train]')\n",
        "        for batch in pbar:\n",
        "            # clear the gradient\n",
        "            optimizer.zero_grad()\n",
        "            # input the mrc and mask\n",
        "            images = batch['image'].to(device)\n",
        "            masks = batch['mask'].to(device)\n",
        "            # prediction\n",
        "            outputs = model(images)\n",
        "            # calculate the loss\n",
        "            loss = criterion(outputs, masks)\n",
        "            # back propagation\n",
        "            loss.backward()\n",
        "            # update weights with SGD\n",
        "            optimizer.step()\n",
        "            # add to sum loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # calculate the IOU\n",
        "            # no gredient here\n",
        "            with torch.no_grad():\n",
        "                # get the predicted label\n",
        "                pred = outputs.argmax(dim = 1)\n",
        "                # valid pixels only\n",
        "                valid = (masks != 255) & (masks != 6)\n",
        "                for one_class in particle_class:\n",
        "                  # valid prediction for one label\n",
        "                  pred_class = (pred == one_class) & valid\n",
        "                  # valid real mask for one label\n",
        "                  mask_class = (masks == one_class) & valid\n",
        "                  # intersection and union\n",
        "                  inter = (pred_class & mask_class).sum().item()\n",
        "                  union = (pred_class | mask_class).sum().item()\n",
        "                  train_iou[one_class]['inter'] += inter\n",
        "                  train_iou[one_class]['union'] += union\n",
        "\n",
        "            # show the batch loss\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        # get the epoch loss\n",
        "        train_loss /= len(train_loader)\n",
        "        # get the epoch IOU\n",
        "        mean_iou = []\n",
        "        for one_class in particle_class:\n",
        "            union = train_iou[one_class]['union']\n",
        "\n",
        "            # make sure union is above 0\n",
        "            if union >0:\n",
        "                iou = train_iou[one_class]['inter'] / train_iou[one_class]['union']\n",
        "                mean_iou.append(iou)\n",
        "        mean_iou = sum(mean_iou) / len(mean_iou) if len(mean_iou) > 0 else 0.0\n",
        "        train_iou = mean_iou\n",
        "\n",
        "        # Validation section\n",
        "        model.eval()\n",
        "        # the loss of the validation set\n",
        "        val_loss = 0.0\n",
        "        # same metrics\n",
        "        val_iou = {stage: {'inter': 0, 'union': 0} for stage in particle_class}\n",
        "\n",
        "        # the mIOU_all\n",
        "        val_iou_all = {stage: {'inter': 0, 'union': 0} for stage in all_class}\n",
        "\n",
        "        # also the dice metrics\n",
        "        val_dice = {stage: {'tp': 0, 'fp': 0, 'fn': 0} for stage in all_class}\n",
        "\n",
        "        # also the dice particle\n",
        "        val_dice_particle = {stage: {'tp': 0, 'fp': 0, 'fn': 0} for stage in particle_class}\n",
        "\n",
        "        # with no gradient\n",
        "        with torch.inference_mode():\n",
        "            # same bar\n",
        "            for batch in tqdm(val_loader, desc = f'Epoch {epoch + 1} / {epochs} [Val]'):\n",
        "                images = batch['image'].to(device)\n",
        "                masks = batch['mask'].to(device)\n",
        "                outputs = model(images)\n",
        "                # calculate the loss\n",
        "                loss = criterion(outputs, masks)\n",
        "                # add to the validation sum loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # same process to get the prediction\n",
        "                pred = outputs.argmax(dim = 1)\n",
        "                # for valid pixels only\n",
        "                valid = (masks != 255) & (masks != 6)\n",
        "                # for each class\n",
        "                for one_class in particle_class:\n",
        "                  # get the valid prediction for this class\n",
        "                  pred_class = (pred == one_class) & valid\n",
        "                  # valid ground truth\n",
        "                  mask_class = (masks == one_class) & valid\n",
        "                  # inter and union\n",
        "                  inter = (pred_class & mask_class).sum().item()\n",
        "                  union = (pred_class | mask_class).sum().item()\n",
        "                  val_iou[one_class]['inter'] += inter\n",
        "                  val_iou[one_class]['union'] += union\n",
        "\n",
        "                  # here we also calculate the dice\n",
        "                  tp_particle = inter\n",
        "                  fp_particle = (pred_class & (~mask_class)).sum().item()\n",
        "                  fn_particle = ((~pred_class) & mask_class).sum().item()\n",
        "                  val_dice_particle[one_class]['tp'] += tp_particle\n",
        "                  val_dice_particle[one_class]['fp'] += fp_particle\n",
        "                  val_dice_particle[one_class]['fn'] += fn_particle\n",
        "\n",
        "                # then, the all class including background\n",
        "                # copy mask\n",
        "                mask2 = masks.clone()\n",
        "                # combine the background label\n",
        "                mask2[mask2 == 5] = 0\n",
        "\n",
        "                # copy the result\n",
        "                pred2 = pred.clone()\n",
        "                # combine the background label\n",
        "                pred2[pred2 == 5] = 0\n",
        "\n",
        "                # the valid\n",
        "                valid_all = (mask2 != 255) & (mask2 != 6)\n",
        "\n",
        "\n",
        "                # calculate the mIOU_all\n",
        "                for one_class in all_class:\n",
        "                    # get the valid prediction\n",
        "                    pred_class_all = (pred2 == one_class) & valid_all\n",
        "                    # valid ground truth\n",
        "                    mask_class_all = (mask2 == one_class) & valid_all\n",
        "                    # get the inter\n",
        "                    inter_all = (pred_class_all & mask_class_all).sum().item()\n",
        "                    # get the union\n",
        "                    union_all = (pred_class_all | mask_class_all).sum().item()\n",
        "                    # record\n",
        "                    val_iou_all[one_class]['inter'] += inter_all\n",
        "                    val_iou_all[one_class]['union'] += union_all\n",
        "\n",
        "                    # here we also calculate the dice\n",
        "                    tp = inter_all\n",
        "                    fp = (pred_class_all & (~mask_class_all)).sum().item()\n",
        "                    fn = ((~pred_class_all) & mask_class_all).sum().item()\n",
        "                    val_dice[one_class]['tp'] += tp\n",
        "                    val_dice[one_class]['fp'] += fp\n",
        "                    val_dice[one_class]['fn'] += fn\n",
        "\n",
        "        # get the validation epoch loss\n",
        "        val_loss /= len(val_loader)\n",
        "        # get the vallidation epoch mIOU_particle\n",
        "        mean_val_iou = []\n",
        "        for one_class in particle_class:\n",
        "            # get the union\n",
        "            union = val_iou[one_class]['union']\n",
        "\n",
        "            # make sure the union is above 0\n",
        "            if union > 0:\n",
        "                iou = val_iou[one_class]['inter'] / val_iou[one_class]['union']\n",
        "                mean_val_iou.append(iou)\n",
        "        mean_val_iou_number = sum(mean_val_iou) / len(mean_val_iou) if len(mean_val_iou) > 0 else 0.0\n",
        "        val_iou = mean_val_iou_number\n",
        "\n",
        "        # get the val dice\n",
        "        mean_val_dice_particle = []\n",
        "        for one_class in particle_class:\n",
        "            tp_p = val_dice_particle[one_class]['tp']\n",
        "            fp_p = val_dice_particle[one_class]['fp']\n",
        "            fn_p = val_dice_particle[one_class]['fn']\n",
        "\n",
        "            # get the denominator first\n",
        "            denominator_p = 2 * tp_p + fp_p + fn_p\n",
        "            # make sure the denominator is above 0\n",
        "            if denominator_p > 0:\n",
        "                dice_p = 2 * tp_p / (2 * tp_p + fp_p + fn_p)\n",
        "                mean_val_dice_particle.append(dice_p)\n",
        "        mean_val_dice_result = sum(mean_val_dice_particle) / len(mean_val_dice_particle) if len(mean_val_dice_particle) > 0 else 0.0\n",
        "        val_dice_p = mean_val_dice_result\n",
        "\n",
        "        # get the mIOU_all\n",
        "        mean_val_iou_all = []\n",
        "        for one_class in all_class:\n",
        "            # get the union\n",
        "            union_all = val_iou_all[one_class]['union']\n",
        "\n",
        "            # make sure the union is above 0\n",
        "            if union_all > 0:\n",
        "               iou_all = val_iou_all[one_class]['inter'] / val_iou_all[one_class]['union']\n",
        "               mean_val_iou_all.append(iou_all)\n",
        "        mean_val_iou_all_number = sum(mean_val_iou_all) / len(mean_val_iou_all) if len(mean_val_iou_all) > 0 else 0.0\n",
        "        val_iou_all = mean_val_iou_all_number\n",
        "\n",
        "        # get the val dice\n",
        "        mean_val_dice = []\n",
        "        for one_class in all_class:\n",
        "            tp = val_dice[one_class]['tp']\n",
        "            fp = val_dice[one_class]['fp']\n",
        "            fn = val_dice[one_class]['fn']\n",
        "\n",
        "            # get the denominator first\n",
        "            denominator = 2 * tp + fp + fn\n",
        "            # make sure the denominator is above 0\n",
        "            if denominator > 0:\n",
        "                dice = 2 * tp / (2 * tp + fp + fn)\n",
        "                mean_val_dice.append(dice)\n",
        "        mean_val_dice_number = sum(mean_val_dice) / len(mean_val_dice) if len(mean_val_dice) > 0 else 0.0\n",
        "        val_dice = mean_val_dice_number\n",
        "\n",
        "        # the cosine annealing update\n",
        "        scheduler.step()\n",
        "\n",
        "        # print summary\n",
        "        print(f'\\n Epoch {epoch + 1}:')\n",
        "        # training set info\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train IoU_particle: {train_iou:.4f}')\n",
        "        # validation set info\n",
        "        print(f'Val Loss: {val_loss:.4f}')\n",
        "        print(f'Val IoU_particle: {val_iou:.4f}, Val Dice_particle: {val_dice_p:.4f}')\n",
        "        print(f'Val IoU_all: {val_iou_all:.4f}, Val Dice_all: {val_dice:.4f}')\n",
        "        print()\n",
        "\n",
        "        # save the weights of the best model (and keep updating)\n",
        "        if val_iou > best_val_iou:\n",
        "            best_val_iou = val_iou\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print('Weights for best model saved')\n",
        "            print()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGkYRD_V7uRd"
      },
      "outputs": [],
      "source": [
        "# training  section\n",
        "def unet_stage():\n",
        "\n",
        "    # load the trainloader and valloader\n",
        "    train_loader, val_loader = create_dataloaders(stages = [1,2,3,4], patch_size = 224, batch_size = 24,\n",
        "                          num_workers = 2, val_split = 0.2)\n",
        "    # for a single batch\n",
        "    for batch in train_loader:\n",
        "      print(f\"batch shape: {batch['image'].shape}\")\n",
        "      # print the statistic\n",
        "      stats = calculate_batch_statistics(batch)\n",
        "      print(f\"Patches with cells: {stats['patches_with_cells']} / {len(batch['image'])}\")\n",
        "      print(f\"Cell pixel ratio: {stats['cell_ratio'] * 100:.2f}%\")\n",
        "      break\n",
        "\n",
        "    # construct the model\n",
        "    model = transunet(img_size = (224, 224))\n",
        "\n",
        "    # device checking\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"device: {device}\")\n",
        "\n",
        "    # training\n",
        "    model = train_model(model, train_loader, val_loader, epochs = 30, device = device, lr = 1e-4,\n",
        "             dice_weight = 0.6, ce_weight = 0.4, save_path = \"/content/drive/MyDrive/Final Project/Image-segmentation-Level-4/TransUnet_seg_weights.pth\")\n",
        "\n",
        "if __name__  == \"__main__\":\n",
        "  unet_stage()\n",
        "  print(\"Training finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJY-vUXz90_q"
      },
      "outputs": [],
      "source": [
        "# release the memory\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUKYEck6nuIf"
      },
      "source": [
        "### Particle picking stage\n",
        "\n",
        "The purpose of this stage is to implement particle picking study based on the above outcome, the saved weights\n",
        "\n",
        "All result will be saved with visualization form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZa3Y4ZPno2d"
      },
      "outputs": [],
      "source": [
        "# import the package\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# normalize first\n",
        "def normalize_data(data):\n",
        "    \"\"\"\n",
        "    mrc normalization\n",
        "\n",
        "    argument:\n",
        "    data: the input data\n",
        "    \"\"\"\n",
        "    # use the quantile normalization\n",
        "    # get the 1st percentile and 99th percentile\n",
        "    p1, p99 = np.percentile(data, [1, 99])\n",
        "    # clip the data\n",
        "    data = np.clip(data, p1, p99)\n",
        "    # then map the data to [0, 1]\n",
        "    data = (data - p1) / (p99 - p1 + 1e-8) # add a 1e-8 to prevent 0\n",
        "    return data\n",
        "\n",
        "def particle_picking(input_file, models, patch_size = 256, stride = 128, device = 'cuda'):\n",
        "  \"\"\"\n",
        "  pick the particle of input files\n",
        "\n",
        "  argument:\n",
        "  file: the input file\n",
        "  models: the used model\n",
        "  patch_size: the size of patch in sliding window sampling\n",
        "  stride: the stride of sliding window sampling\n",
        "  device: cuda or cpu\n",
        "  \"\"\"\n",
        "\n",
        "  # make sure 2D file\n",
        "  assert input_file.ndim == 2, \"Input file should be 2D\"\n",
        "\n",
        "  # normalize the target file\n",
        "  norm_file = normalize_data(input_file)\n",
        "\n",
        "  # the 224 patch size is not divisible, we need padding\n",
        "  def need_padding(origin_image, patch_size, stride):\n",
        "      \"\"\"\n",
        "      add padding to the image for slding window sampling\n",
        "\n",
        "      argument:\n",
        "      origin_image: the input image\n",
        "      patch_size: the size of patch in sliding window sampling\n",
        "      stride: the stride of sliding window sampling\n",
        "      \"\"\"\n",
        "      # original size\n",
        "      h, w = origin_image.shape\n",
        "      # calculate the required length\n",
        "      # Needed length = stride * n + patch, n to Z\n",
        "      need_h = ((h - patch_size) // stride + 1) * stride + patch_size\n",
        "      need_w = ((w - patch_size) // stride + 1) * stride + patch_size\n",
        "      # calculate padding length\n",
        "      pad_h = need_h - h\n",
        "      pad_w = need_w - w\n",
        "      # pad with black, like the black border\n",
        "      new_image = np.pad(origin_image, ((0, pad_h), (0, pad_w)), mode = 'constant', constant_values = 0)\n",
        "      return new_image, pad_h, pad_w\n",
        "\n",
        "  # pad this micrograph\n",
        "  norm_file, pad_h, pad_w = need_padding(norm_file, patch_size, stride)\n",
        "\n",
        "  # get the file shape\n",
        "  h, w = norm_file.shape\n",
        "\n",
        "  # define the global info\n",
        "  classes = 7\n",
        "  logit_glob = torch.zeros(classes, h, w, device = device, dtype = torch.float32)\n",
        "  count_glob = torch.zeros(h, w, device = device, dtype = torch.float32)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for y in range(0, h - patch_size + 1, stride):\n",
        "          for x in range(0, w - patch_size + 1, stride):\n",
        "              # get the patch\n",
        "              patch = norm_file[y:y + patch_size, x:x + patch_size]\n",
        "              # convert to tensor\n",
        "              # normalization\n",
        "              # convert to float\n",
        "              patch  = patch.astype(np.float32)\n",
        "\n",
        "              # stack up to rgb\n",
        "              patch = np.stack([patch] * 3, axis = 2) # (H, W, 3)\n",
        "\n",
        "              # use Imagenet normalization\n",
        "              mean = np.array([0.485, 0.456, 0.406])\n",
        "              std = np.array([0.229, 0.224, 0.225])\n",
        "              patch = (patch - mean) / std\n",
        "\n",
        "              # change the (H, W, 3) to (3, H, W)\n",
        "              patch = patch.transpose(2, 0, 1)\n",
        "\n",
        "              # convert to tensor\n",
        "              # now the patch size is (h, w), we want [batch_size, n_channels, h, w]\n",
        "              patch_tensor = torch.from_numpy(patch[None]).float().to(device)\n",
        "\n",
        "              # fit for logit [C,H,W]\n",
        "              result = models(patch_tensor)[0]\n",
        "              # get probs\n",
        "              probs = F.softmax(result, dim = 0)\n",
        "              # add to the globe\n",
        "              logit_glob[:, y:y + patch_size, x:x + patch_size] += probs\n",
        "              # add to the count\n",
        "              count_glob[y:y + patch_size, x:x + patch_size] += 1.0\n",
        "\n",
        "  # make sure the count is no less than 1\n",
        "  count_glob = torch.clamp(count_glob, min = 1.0)\n",
        "  # get the mean logit, [C,H,W] / [1,H,W]\n",
        "  avg_glob = logit_glob / count_glob.unsqueeze(0)\n",
        "\n",
        "  # get the prediction label\n",
        "  pred_label = torch.argmax(avg_glob, dim = 0)\n",
        "\n",
        "  # cut the padding\n",
        "  if pad_h > 0 or pad_w > 0:\n",
        "      pred_label = pred_label[:-pad_h, :-pad_w]\n",
        "\n",
        "  # get the boolean from foreground mask\n",
        "  pred1 = (pred_label == 1)\n",
        "\n",
        "  pred2 = (pred_label == 2)\n",
        "\n",
        "  pred3 = (pred_label == 3)\n",
        "\n",
        "  pred4 = (pred_label == 4)\n",
        "\n",
        "  return pred1.cpu().numpy(), pred2.cpu().numpy(), pred3.cpu().numpy(), pred4.cpu().numpy(), pred_label.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWz5kqQioK9r"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "# implement the model\n",
        "# specify the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "weight_path = '/content/drive/MyDrive/Final Project/Image-segmentation-Level-4/TransUnet_seg_weights.pth'\n",
        "\n",
        "# create TransUnet\n",
        "model = transunet(img_size = (224, 224)).to(device)\n",
        "# load existed weights\n",
        "model.load_state_dict(torch.load(weight_path, map_location = device))\n",
        "# eval mode\n",
        "model.eval()\n",
        "\n",
        "root_path = Path('/content/drive/MyDrive/Final Project/Dataset-processed')\n",
        "mask_base = Path('/content/drive/MyDrive/Final Project/Image-segmentation-Level-4')\n",
        "\n",
        "# requied mapping\n",
        "stages = {'stageI': 1, 'stageII': 2, 'stageIII': 3, 'stageIV': 4}\n",
        "\n",
        "# define the path to save result\n",
        "out_dir = mask_base / 'TransUnet_result_on_MRC'\n",
        "os.makedirs(out_dir, exist_ok = True)\n",
        "\n",
        "# random seed\n",
        "random.seed(2025)\n",
        "\n",
        "# extract files\n",
        "for stage in stages.keys():\n",
        "    print(f\"Now extracting from {stage}\")\n",
        "    # connect to get the stage folder path\n",
        "    stage_path = root_path / stage\n",
        "    # find tif files\n",
        "    all_files = [f for f in os.listdir(stage_path) if f.endswith('.mrc')]\n",
        "    # print choose files\n",
        "    all_files = random.sample(all_files, min(100, len(all_files)))\n",
        "    print(f\"chosen file: {all_files}\")\n",
        "    print()\n",
        "\n",
        "    # make stage folder\n",
        "    stage_dir = out_dir / stage\n",
        "    os.makedirs(stage_dir, exist_ok = True)\n",
        "\n",
        "    # for one file\n",
        "    for one_file in all_files:\n",
        "        results = {}\n",
        "        # get the file path\n",
        "        file_path = stage_path / one_file\n",
        "        # read the file\n",
        "        with mrcfile.open(file_path) as mrc:\n",
        "          # convert to float\n",
        "          file_data = mrc.data.astype(np.float32)\n",
        "\n",
        "        # fit the mrc file into model\n",
        "        pred1, pred2, pred3, pred4, pred_mask = particle_picking(file_data, model, patch_size = 224, stride = 112, device = device)\n",
        "\n",
        "        # the percentage of multi-stage mask\n",
        "        for single_stage, corresponding_mask in enumerate([pred1, pred2, pred3, pred4], start = 1):\n",
        "            results[single_stage] = float(corresponding_mask.mean() * 100.0)\n",
        "\n",
        "        # draw the plot\n",
        "        norm_origin = normalize_data(file_data.squeeze())\n",
        "        # to [H,W,3]\n",
        "        rgb_origin = np.stack([norm_origin] * 3, axis = -1)\n",
        "        # the mark overlay\n",
        "        overlay = rgb_origin.copy()\n",
        "        # four colors\n",
        "        # stage 1 use blue\n",
        "        overlay[pred1] = [0.0, 0.0, 1.0]\n",
        "        # stage 2 use red\n",
        "        overlay[pred2] = [1.0, 0.0, 0.0]\n",
        "        # stage 3 use yellow\n",
        "        overlay[pred3] = [1.0, 1.0, 0.0]\n",
        "        # stage 4 use pink\n",
        "        overlay[pred4] = [1.0, 0.0, 1.0]\n",
        "        # mix\n",
        "        integrated = 0.6 * rgb_origin + 0.4 * overlay\n",
        "\n",
        "        # file name\n",
        "        name = file_path.stem\n",
        "\n",
        "        # save file\n",
        "        save_path = stage_dir / f\"{name}_modelpred.png\"\n",
        "        plt.imsave(save_path.as_posix(), integrated)\n",
        "\n",
        "        # print statistic\n",
        "        print(f\"File name: {file_path}\")\n",
        "        for the_stage_number, the_percentage in results.items():\n",
        "            print(f\"{the_stage_number} percentage: {the_percentage:.2f}%\")\n",
        "        print(f\"Saved to {save_path}\")\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}