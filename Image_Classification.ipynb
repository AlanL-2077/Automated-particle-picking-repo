{
  "cells": [
    {
      "metadata": {
        "id": "1ce64a745c7a2d41"
      },
      "cell_type": "markdown",
      "source": [
        "## EMATM0047: Data Science Project\n",
        "---\n",
        "### Code Section S1: Image classification\n",
        "---\n",
        "#### Author: Siyu Liu\n",
        "#### Faculty of Engineering\n",
        "#### University of Bristol\n",
        "</br>\n",
        "\n",
        "\n",
        "#### Input:\n",
        "1. The raw pictures under the Cryo-EM that are stored via the .mrc files.\n",
        "2. The corresponding artificial labelled masks, stored via the .svg files.\n",
        "\n",
        "#### Operation:\n",
        "1. Import and split the dataset with 5-fold cross validation\n",
        "\n",
        "#### Output:\n",
        "1. The image classification results under the circumstance of freezing backbone\n",
        "\n",
        "2. The image classification results under the circumstance of fine-tuned variant"
      ],
      "id": "1ce64a745c7a2d41"
    },
    {
      "metadata": {
        "id": "dfc437f63d6e9927"
      },
      "cell_type": "markdown",
      "source": [
        "Define the path of root dictionary and its auxiliary stage folders"
      ],
      "id": "dfc437f63d6e9927"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bWSeSUOlS9T_"
      },
      "id": "bWSeSUOlS9T_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the root directory\n",
        "from pathlib import Path\n",
        "\n",
        "Root = Path('/content/drive/MyDrive/Final Project/Dataset-processed')\n",
        "Stages = ['stageI', 'stageII', 'stageIII', 'stageIV'] # four stages"
      ],
      "metadata": {
        "id": "70l9uIys_yIE"
      },
      "id": "70l9uIys_yIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ff37a81ea45708e0"
      },
      "cell_type": "markdown",
      "source": [
        "Import the file"
      ],
      "id": "ff37a81ea45708e0"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-12T13:04:41.065947Z",
          "start_time": "2025-06-12T13:04:41.052160Z"
        },
        "id": "b1ccc62785291e5c"
      },
      "cell_type": "code",
      "source": [
        "model_sample = []\n",
        "\n",
        "for label, stage in enumerate(Stages): # for each stage\n",
        "    for mrc in (Root / stage).glob('*.mrc'): # for all mrc file in one stage\n",
        "        name = mrc.stem\n",
        "        # mrc and svg have same stem\n",
        "        related_svg = mrc.parent / f\"{name}.svg\"\n",
        "        model_sample.append((str(mrc), str(related_svg), label))\n",
        "print(f\"{len(model_sample)} samples in total\")"
      ],
      "id": "b1ccc62785291e5c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b253b659e65c08a2"
      },
      "cell_type": "markdown",
      "source": [
        "Split the dataset with stratified 5 folds cross validation"
      ],
      "id": "b253b659e65c08a2"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-12T13:04:43.013218Z",
          "start_time": "2025-06-12T13:04:42.982559Z"
        },
        "id": "c3e9b81717bd97b3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import json\n",
        "\n",
        "# 1 to 4\n",
        "labels = [s[2] for s in model_sample]\n",
        "\n",
        "# create the new dictionary\n",
        "output_dir = (Root / '5fs CV')\n",
        "output_dir.mkdir(exist_ok = True)\n",
        "\n",
        "# use StratifiedKFold() function\n",
        "cv_method = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 2025)\n",
        "# split to (train, test), same distribution\n",
        "cv_result = cv_method.split(model_sample, labels)\n",
        "\n",
        "# save 5 folds to json\n",
        "for fold, (train, test) in enumerate(cv_result):\n",
        "    # save the file path for training\n",
        "    json.dump([model_sample[i] for i in train], open(output_dir / f\"fold{fold}_train.json\", \"w\"))\n",
        "    # save the file path for testing\n",
        "    json.dump([model_sample[i] for i in test], open(output_dir / f\"fold{fold}_test.json\", \"w\"))"
      ],
      "id": "c3e9b81717bd97b3",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the needed packages in advance"
      ],
      "metadata": {
        "id": "bX9p8pPXCH-x"
      },
      "id": "bX9p8pPXCH-x"
    },
    {
      "cell_type": "code",
      "source": [
        "# need mrcfile to process .mrc and cairosvg to process .svg\n",
        "try:\n",
        "  import mrcfile\n",
        "  print(\"Yes\")\n",
        "except:\n",
        "  print(\"No\")\n",
        "  !pip install mrcfile\n",
        "  !pip install cairosvg\n",
        "  print(\"mrcfile and cairosvg installed, continue\")"
      ],
      "metadata": {
        "id": "0UCOZI1WCu64"
      },
      "id": "0UCOZI1WCu64",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ab07f61c38b72921"
      },
      "cell_type": "markdown",
      "source": [
        "Use the .svg file to create the ROI patch on the original micrograph"
      ],
      "id": "ab07f61c38b72921"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-14T16:11:34.054477Z",
          "start_time": "2025-06-14T16:11:34.044377Z"
        },
        "id": "e953ce87264dc048"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import mrcfile\n",
        "import cv2\n",
        "from cairosvg import svg2png\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ROIprocess(Dataset):\n",
        "    \"\"\"\n",
        "    This part use the train and test json file as input,\n",
        "    with the following operations:\n",
        "\n",
        "    1. read the .mrc file, map it to [0,255] for the grayscale\n",
        "    2. read the .svg file\n",
        "    3. retrieve the external contours from the mask to get the ROI(region of interest)\n",
        "    4. crop the original .mrc image with ROI\n",
        "    5. resize the cropped .mrc image to 224x224\n",
        "    6. normalize the cropped .mrc image\n",
        "    7. duplicate it to rgb to satisfy the model input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_path, image_size = 224):\n",
        "        \"\"\"\n",
        "        initialization\n",
        "\n",
        "        argument:\n",
        "        json_path: the json file saved before\n",
        "        image_size: the size of the output image\n",
        "        \"\"\"\n",
        "        self.items = json.load(open(json_path))\n",
        "        self.sz = image_size\n",
        "        self.mrc_cache = {}\n",
        "        self.svg_cache = {}\n",
        "        # preload data\n",
        "        self.pre_load()\n",
        "\n",
        "    # mrc importation function\n",
        "    @staticmethod\n",
        "    def mrc_import(path: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        import and normalize the .mrc file\n",
        "        \"\"\"\n",
        "        with mrcfile.open(path) as mrc:\n",
        "            arr = mrc.data.astype(np.float32) # float for accuracy\n",
        "\n",
        "        # quantile normalization\n",
        "        p1, p99 = np.percentile(arr, [1, 99])\n",
        "        arr = np.clip(arr, p1, p99)\n",
        "        arr = ((arr - p1) / max(p99 - p1, 1e-5) * 255)\n",
        "\n",
        "        # return to uint8, standard greyscale\n",
        "        return arr.astype(np.uint8)\n",
        "\n",
        "    # the function convert svg to binary mask\n",
        "    @staticmethod\n",
        "    def svg2mask(svg_path: str, H: int, W: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        this function converts the svg to png, for convinence\n",
        "\n",
        "        argument:\n",
        "        svg_path: the path of svg file\n",
        "        H: the height of the image\n",
        "        W: the width of the image\n",
        "\n",
        "        \"\"\"\n",
        "        # switch to png\n",
        "        png = svg2png(url = svg_path, output_width = W, output_height = H)\n",
        "        # must pack png as 1D numpy array\n",
        "        png_convert = np.frombuffer(png, dtype = np.uint8)\n",
        "        # so that we can convert it to greyscale\n",
        "        mask = cv2.imdecode(png_convert, cv2.IMREAD_GRAYSCALE)\n",
        "        # greyscale to binary, 0 for black and 255 for white\n",
        "        return (mask > 5).astype(np.uint8) * 255\n",
        "\n",
        "    def pre_load(self):\n",
        "      \"\"\"\n",
        "      this function is used to pre_load the file to save time\n",
        "      \"\"\"\n",
        "      for idx, (mrc_path, svg_path, label) in enumerate(tqdm(self.items)):\n",
        "        # save mrc\n",
        "        self.mrc_cache[idx] = self.mrc_import(mrc_path)\n",
        "        # get image size\n",
        "        image = self.mrc_cache[idx]\n",
        "        # save svg\n",
        "        self.svg_cache[idx] = self.svg2mask(svg_path, *image.shape)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        process the input\n",
        "\n",
        "        argument:\n",
        "        idx: the index of the sample\n",
        "        \"\"\"\n",
        "        # (str(mrc), str(related_svg), label)\n",
        "        mrc_path, svg_path, label = self.items[idx]\n",
        "\n",
        "        # get the mrc and svg from cache\n",
        "        image = self.mrc_cache[idx]\n",
        "        mask = self.svg_cache[idx]\n",
        "\n",
        "        # find the cell mask, white dot\n",
        "        # cv2.RETR_EXTERNAL outer contour only, cv2.CHAIN_APPROX_SIMPLE key points (4) on the contour only\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        # no cell\n",
        "        if not contours:\n",
        "            # to (224,224)\n",
        "            roi = cv2.resize(image, (self.sz, self.sz), interpolation = cv2.INTER_AREA)\n",
        "        else:\n",
        "            x_list = []\n",
        "            y_list = []\n",
        "            for contour in contours:\n",
        "                # minimum bounding rectangle of cell mask\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                # four coordinates of rect\n",
        "                x_list.extend([x, x + w])\n",
        "                y_list.extend([y, y + h])\n",
        "            # get the extreme value\n",
        "            x0, x1 = min(x_list), max(x_list)\n",
        "            y0, y1 = min(y_list), max(y_list)\n",
        "            # get the region of interest(ROI)\n",
        "            w = x1 - x0\n",
        "            h = y1 - y0\n",
        "\n",
        "            # give some padding, not to tight\n",
        "            pad = int(max(w, h) * 0.05)\n",
        "            # make sure still in the whole image\n",
        "            H, W = image.shape\n",
        "            # smallest to 0\n",
        "            x0 = max(0, x0 - pad)\n",
        "            y0 = max(0, y0 - pad)\n",
        "            # highest to h,w\n",
        "            x1 = min(W, x1 + pad)\n",
        "            y1 = min(H, y1 + pad)\n",
        "\n",
        "            # cut the ROI on mrc\n",
        "            roi = image[y0: y1, x0: x1]\n",
        "\n",
        "            # ROI must be a square\n",
        "            h_roi, w_roi = roi.shape\n",
        "            if h_roi != w_roi:\n",
        "                # calculate the compensation\n",
        "                difference = abs(h_roi - w_roi)\n",
        "                pad_a = difference // 2\n",
        "                pad_b = difference - pad_a\n",
        "                # pad h\n",
        "                if h_roi < w_roi :\n",
        "                    roi = np.pad(roi, ((pad_a, pad_b), (0, 0)), mode = 'edge')\n",
        "                # pad w\n",
        "                else:\n",
        "                    roi = np.pad(roi, ((0, 0), (pad_a, pad_b)), mode = 'edge')\n",
        "\n",
        "            # resize the ROI to 224*224\n",
        "            # ROI > 244, use cv2.INTER_AREA to keep detail\n",
        "            # ROI < 224, use cv2.INTER_LINEAR to keep smooth\n",
        "            interp_choice = cv2.INTER_LINEAR if min(roi.shape) < self.sz else cv2.INTER_AREA\n",
        "            roi = cv2.resize(roi, (self.sz, self.sz), interpolation = interp_choice) # roi resize\n",
        "\n",
        "        # normalization\n",
        "        # [0,1]\n",
        "        roi = roi.astype(np.float32) / 255\n",
        "\n",
        "        # greyscale to rgb, [3,224,224]\n",
        "        roi_rgb = np.stack([roi, roi, roi], axis = 0)\n",
        "\n",
        "        # imagenet normalization\n",
        "        mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
        "        std  = np.array([0.229, 0.224, 0.225])[:, None, None]\n",
        "        roi_rgb = (roi_rgb - mean) / std\n",
        "\n",
        "        # to tensor\n",
        "        roi_rgb = torch.from_numpy(roi_rgb)\n",
        "\n",
        "        # label to tensor, discrete number\n",
        "        label_tensor = torch.tensor(label, dtype = torch.long)\n",
        "\n",
        "        return roi_rgb, label_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)"
      ],
      "id": "e953ce87264dc048",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f93449a784fe341e"
      },
      "cell_type": "markdown",
      "source": [
        "Construct the Dataloader for each fold"
      ],
      "id": "f93449a784fe341e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-14T16:11:43.839493Z",
          "start_time": "2025-06-14T16:11:43.833279Z"
        },
        "id": "4db671a7c1322afd"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "batch_size = 16\n",
        "num_workers = 2\n",
        "\n",
        "# root and 5-fold folders\n",
        "Root = Path('/content/drive/MyDrive/Final Project/Dataset-processed')\n",
        "folder = (Root / '5fs CV')\n",
        "\n",
        "def make_dataloader(fold_idx: int):\n",
        "    \"\"\"\n",
        "    This part links to the above function to construct the dataloader.\n",
        "\n",
        "    argument:\n",
        "    fold_idx: the index of the fold\n",
        "    \"\"\"\n",
        "\n",
        "    # import the fold\n",
        "    train_json = folder / f\"fold{fold_idx}_train.json\"\n",
        "    test_json = folder / f\"fold{fold_idx}_test.json\"\n",
        "\n",
        "    # get tensor\n",
        "    train_dataset = ROIprocess(train_json, image_size = 224)\n",
        "    test_dataset = ROIprocess(test_json, image_size = 224)\n",
        "\n",
        "    # allocate a random seed for each fold\n",
        "    gen = torch.Generator().manual_seed(2025 + fold_idx)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, generator = gen,\n",
        "                    shuffle = True, num_workers = num_workers, pin_memory = True, drop_last = True)\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size = batch_size, generator = gen,\n",
        "                    shuffle = False, num_workers = num_workers, pin_memory = True)\n",
        "\n",
        "    print(f\"Processing fold {fold_idx}, get {len(train_dataset)} training samples, {len(test_dataset)} test samples\")\n",
        "\n",
        "    return train_dataloader, test_dataloader"
      ],
      "id": "4db671a7c1322afd",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "specift the device, cuda or cpu"
      ],
      "metadata": {
        "id": "JXGKBca0YoV1"
      },
      "id": "JXGKBca0YoV1"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-14T16:11:46.655704Z",
          "start_time": "2025-06-14T16:11:46.614810Z"
        },
        "id": "9c9e447a78588071"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# specify the device, use GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "id": "9c9e447a78588071",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# create all dataloader here, shorten the training time\n",
        "all_dataloader = []\n",
        "\n",
        "for fold_idx in range(5):\n",
        "    print(f\"Working on the fold {fold_idx}\")\n",
        "    train_dataloader, test_dataloader = make_dataloader(fold_idx)\n",
        "    all_dataloader.append((train_dataloader, test_dataloader))\n",
        "    print(f\"Finish the fold {fold_idx}\")"
      ],
      "metadata": {
        "id": "9hrkxniVzyXl"
      },
      "id": "9hrkxniVzyXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e11850600e0bb72"
      },
      "cell_type": "markdown",
      "source": [
        "The model construction and training section"
      ],
      "id": "e11850600e0bb72"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-14T16:20:29.373712Z",
          "start_time": "2025-06-14T16:12:16.063066Z"
        },
        "id": "fa6c360a0d58fb2a"
      },
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# store the best weight\n",
        "fold_accuracy = []\n",
        "best_weight_path = Root / 'best_weight'\n",
        "best_weight_path.mkdir(exist_ok = True)\n",
        "\n",
        "# one fold pair\n",
        "for fold_idx, (train_dataloader, test_dataloader) in enumerate(all_dataloader):\n",
        "\n",
        "    # introduce the model\n",
        "    model = models.mobilenet_v2(weights = 'IMAGENET1K_V1')\n",
        "\n",
        "    # we want 4 classes, original is 1000\n",
        "    model.classifier[1] = nn.Linear(model.last_channel, 4)\n",
        "\n",
        "    # freeze all gradient update except the last\n",
        "    for name, p in model.named_parameters():\n",
        "        if not name.startswith('classifier.1'):\n",
        "            p.requires_grad = False\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # the lost function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # AdamW for optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    epochs = 5\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # model training\n",
        "        model.train()\n",
        "        for x, y in tqdm(train_dataloader, desc = f\"Fold {fold_idx}, epoch {epoch} [Train]\"):\n",
        "            # non-blocking + pin_memory, faster\n",
        "            x = x.to(device, non_blocking = True).float()\n",
        "            y = y.to(device, non_blocking = True).long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(x)\n",
        "\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        # model evaluation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(test_dataloader, desc = f\"Fold {fold_idx}, epoch {epoch} [Val]\"):\n",
        "                # non-blocking + pin_memory, faster\n",
        "                x = x.to(device, non_blocking = True).float()\n",
        "                y = y.to(device, non_blocking = True).long()\n",
        "                # predicted label, (batch_size,)\n",
        "                pred = model(x).argmax(1)\n",
        "                # how many are correct\n",
        "                correct += (pred == y).sum().item()\n",
        "                # how many in total\n",
        "                total += y.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"The fold {fold_idx}, Epoch {epoch:02d} / {epochs}, validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # save the weight of the current best epoch accuracy\n",
        "        if accuracy > best_accuracy:\n",
        "          best_accuracy = accuracy\n",
        "          weight_path = best_weight_path / f'fold{fold_idx}_best.pth'\n",
        "          torch.save(model.state_dict(), weight_path)\n",
        "          print(f\"Best model updated, save to {weight_path}\")\n",
        "\n",
        "    # best epoch accuracy\n",
        "    print(f\"The best accuracy of fold {fold_idx} is: {best_accuracy:.4f} \\n\")\n",
        "    fold_accuracy.append(best_accuracy)\n",
        "\n",
        "print(f\"The mean accuracy of 5 folds is {np.mean(fold_accuracy)}\")"
      ],
      "id": "fa6c360a0d58fb2a",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the conufusion matrix of the best epoch of each fold"
      ],
      "metadata": {
        "id": "aPRwPjpJvO4b"
      },
      "id": "aPRwPjpJvO4b"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def confusion_print(model, test_dataloader, classes, fold_idx, save_path = None):\n",
        "  \"\"\"\n",
        "  this function prints the confusion matrix according to weights saved before\n",
        "\n",
        "  argument:\n",
        "  model: MobileNet V2\n",
        "  test_dataloader: the test dataloader\n",
        "  classes: 4\n",
        "  fold_idx: the index of the fold\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_dataloader:\n",
        "      x = x.to(device).float()\n",
        "      y = y.to(device).long()\n",
        "      # the predicted label\n",
        "      pred = model(x).argmax(1)\n",
        "      # add to list\n",
        "      preds.append(pred.cpu().numpy())\n",
        "      # ground truth to list\n",
        "      labels.append(y.cpu().numpy())\n",
        "  # concat list to one-dimensional vector\n",
        "  y_pred = np.concatenate(preds)\n",
        "  y_true = np.concatenate(labels)\n",
        "\n",
        "  # generate confusion matrix\n",
        "  confusion = confusion_matrix(y_true, y_pred, labels = list(range(len(classes))))\n",
        "  # display it\n",
        "  output = ConfusionMatrixDisplay(confusion, display_labels = classes)\n",
        "  # show the confusion matrix\n",
        "  fig, ax = plt.subplots(figsize = (6, 6))\n",
        "  output.plot(ax = ax, cmap = 'Blues', colorbar = False)\n",
        "  ax.set_title(f\"Fold_{fold_idx + 1}_Confusion_Matrix\")\n",
        "\n",
        "  # save the picture\n",
        "  save_path = save_path / f'fold{fold_idx + 1}_confusion matrix.png'\n",
        "  plt.savefig(save_path)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "xgpIxzKfk8tY"
      },
      "id": "xgpIxzKfk8tY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['class 1', 'class 2', 'class 3', 'class 4']\n",
        "best_weight_path = Root / 'best_weight'\n",
        "\n",
        "for fold_idx, (train_dataloader, test_dataloader) in enumerate(all_dataloader):\n",
        "    # same model but no weights\n",
        "    model = models.mobilenet_v2(weights = None)\n",
        "    # still 4 classes\n",
        "    model.classifier[1] = nn.Linear(model.last_channel, 4)\n",
        "    # load the best weight\n",
        "    model.load_state_dict(torch.load(best_weight_path / f'fold{fold_idx}_best.pth'))\n",
        "    model.to(device)\n",
        "\n",
        "    confusion_print(model, test_dataloader, classes, fold_idx, save_path = Root / 'best_weight')"
      ],
      "metadata": {
        "id": "FrcsIQC_w6hJ"
      },
      "id": "FrcsIQC_w6hJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The above model is the version which freezes all layers except the final classifier\n",
        "# Now there will be a fine-tune model\n",
        "\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "fold_accuracy = []\n",
        "best_weight_path = Root / 'best_weight_finetune'\n",
        "best_weight_path.mkdir(exist_ok = True)\n",
        "\n",
        "\n",
        "# one fold pair\n",
        "for fold_idx, (train_dataloader, test_dataloader) in enumerate(all_dataloader):\n",
        "\n",
        "    # introduce the model\n",
        "    model = models.mobilenet_v2(weights = 'IMAGENET1K_V1')\n",
        "    # we want 4 classes\n",
        "    model.classifier[1] = nn.Linear(model.last_channel, 4)\n",
        "    model.to(device)\n",
        "\n",
        "    # unfreeze all\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    best_accuracy = 0\n",
        "    epochs = 5\n",
        "\n",
        "    # the lost function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5, weight_decay = 1e-4)\n",
        "\n",
        "    # add a scheduler, the cosine annealing\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs, eta_min = 0.0)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # model training phase\n",
        "        model.train()\n",
        "        for x, y in tqdm(train_dataloader, desc = f\"Fold {fold_idx}, epoch {epoch} [Train]\"):\n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device).long()\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # model evaluation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(test_dataloader, desc = f\"Fold {fold_idx}, epoch {epoch} [Val]\"):\n",
        "                x = x.to(device).float()\n",
        "                y = y.to(device).long()\n",
        "                # get predicted label\n",
        "                pred = model(x).argmax(1)\n",
        "                # how many are correct\n",
        "                correct += (pred == y).sum().item()\n",
        "                # how many in total\n",
        "                total += y.size(0)\n",
        "        accuracy = correct / total\n",
        "        print(f\"The fold {fold_idx}, Epoch {epoch:02d} / {epochs}, validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # save the weight of the current best accuracy\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            weight_path = best_weight_path / f'fold{fold_idx}_best.pth'\n",
        "            torch.save(model.state_dict(), weight_path)\n",
        "            print(f\"Best model updated, save to {weight_path}\")\n",
        "\n",
        "    # best epoch accuracy\n",
        "    print(f\"The best accuracy of fold {fold_idx} is: {best_accuracy:.4f} \\n\")\n",
        "    fold_accuracy.append(best_accuracy)\n",
        "\n",
        "# mean accuracy\n",
        "print(f\"The mean accuracy of 5 folds is {np.mean(fold_accuracy):.4f} ± {np.std(fold_accuracy):.4f}\")"
      ],
      "metadata": {
        "id": "ksfEGxM6y8uU"
      },
      "id": "ksfEGxM6y8uU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['class 1', 'class 2', 'class 3', 'class 4']\n",
        "best_weight_path = Root / 'best_weight_finetune'\n",
        "\n",
        "for fold_idx, (train_dataloader, test_dataloader) in enumerate(all_dataloader):\n",
        "    # same model but no weights\n",
        "    model = models.mobilenet_v2(weights = None)\n",
        "    # still 4 classes\n",
        "    model.classifier[1] = nn.Linear(model.last_channel, 4)\n",
        "    # load the best weight\n",
        "    model.load_state_dict(torch.load(best_weight_path / f'fold{fold_idx}_best.pth'))\n",
        "    model.to(device)\n",
        "\n",
        "    confusion_print(model, test_dataloader, classes, fold_idx, save_path = Root / 'best_weight_finetune')"
      ],
      "metadata": {
        "id": "h1SASZzg8TNw"
      },
      "id": "h1SASZzg8TNw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explainable analysis with Grad-CAM\n",
        "\n",
        "import the package first"
      ],
      "metadata": {
        "id": "EYaQzb9VX8aN"
      },
      "id": "EYaQzb9VX8aN"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from pytorch_grad_cam import GradCAM\n",
        "  print(\"Yes\")\n",
        "except:\n",
        "  print(\"No\")\n",
        "  # install only, don't change CUDA and cuDNN\n",
        "  !pip install --no-deps grad-cam == 1.5.5\n",
        "  !pip install --no-deps ttach == 0.0.3\n",
        "  print(\"grad-cam installed\")\n",
        "  from pytorch_grad_cam import GradCAM\n",
        "  print(\"grad-cam imported\")"
      ],
      "metadata": {
        "id": "c0aNszVtX7i9"
      },
      "id": "c0aNszVtX7i9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the same process of how the ROI is made"
      ],
      "metadata": {
        "id": "JadfnsZir93F"
      },
      "id": "JadfnsZir93F"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import mrcfile\n",
        "import cv2\n",
        "from cairosvg import svg2png\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def roi_process(mrc_path, svg_path, label, img_size = 224):\n",
        "    \"\"\"\n",
        "    This function repeats the process of getting the ROI\n",
        "\n",
        "    argument:\n",
        "    mrc_path: the imput mrc path\n",
        "    svg_path: the input svg path\n",
        "    img_size: the image size\n",
        "    device: the device, cuda or cpu\n",
        "    \"\"\"\n",
        "    # import and normalize mrc\n",
        "    with mrcfile.open(mrc_path, permissive = True) as mrc:\n",
        "        mrc_data = mrc.data.astype(np.float32)\n",
        "\n",
        "    # quantile normalization\n",
        "    p1, p99 = np.percentile(mrc_data, [1, 99])\n",
        "    mrc_data = np.clip(mrc_data, p1, p99)\n",
        "    image = ((mrc_data - p1) / max(p99 - p1, 1e-5) * 255).astype(np.uint8)\n",
        "\n",
        "    # binary mask\n",
        "    W, H = mrc_data.shape\n",
        "    png = svg2png(url = svg_path, output_width = W, output_height = H)\n",
        "    # must pack png as 1D numpy array\n",
        "    png_convert = np.frombuffer(png, dtype = np.uint8)\n",
        "    # so that we can convert it to greyscale\n",
        "    mask = cv2.imdecode(png_convert, cv2.IMREAD_GRAYSCALE)\n",
        "    # greyscale to binary, 0 for black and 255 for white\n",
        "    mask = (mask > 5).astype(np.uint8) * 255\n",
        "\n",
        "    # ROI\n",
        "    # find the cell mask, white dot\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # no cell\n",
        "    if not contours:\n",
        "        # to (224,224)\n",
        "        roi = cv2.resize(image, (img_size, img_size), interpolation = cv2.INTER_AREA)\n",
        "    else:\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        for contour in contours:\n",
        "            # minimum bounding rectangle of cell mask\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            # four coordinates of rect\n",
        "            x_list.extend([x, x + w])\n",
        "            y_list.extend([y, y + h])\n",
        "        # get the extreme value\n",
        "        x0, x1 = min(x_list), max(x_list)\n",
        "        y0, y1 = min(y_list), max(y_list)\n",
        "        # get the region of interest(ROI)\n",
        "        w = x1 - x0\n",
        "        h = y1 - y0\n",
        "\n",
        "        # give some padding, not to tight\n",
        "        pad = int(max(w, h) * 0.05)\n",
        "        # make sure still in the whole image\n",
        "        H, W = image.shape\n",
        "        # smallest to 0\n",
        "        x0 = max(0, x0 - pad)\n",
        "        y0 = max(0, y0 - pad)\n",
        "        # highest to h,w\n",
        "        x1 = min(W, x1 + pad)\n",
        "        y1 = min(H, y1 + pad)\n",
        "\n",
        "        # cut the ROI on mrc\n",
        "        roi = image[y0: y1, x0: x1]\n",
        "\n",
        "        # ROI must be a square\n",
        "        h_roi, w_roi = roi.shape\n",
        "        if h_roi != w_roi:\n",
        "            # calculate the compensation\n",
        "            difference = abs(h_roi - w_roi)\n",
        "            pad_a = difference // 2\n",
        "            pad_b = difference - pad_a\n",
        "            # pad h\n",
        "            if h_roi < w_roi :\n",
        "                roi = np.pad(roi, ((pad_a, pad_b), (0, 0)), mode = 'edge')\n",
        "            # pad w\n",
        "            else:\n",
        "                roi = np.pad(roi, ((0, 0), (pad_a, pad_b)), mode = 'edge')\n",
        "\n",
        "        # resize the ROI to 224*224\n",
        "        # ROI > 244, use cv2.INTER_AREA to keep detail\n",
        "        # ROI < 224, use cv2.INTER_LINEAR to keep smooth\n",
        "        interp_choice = cv2.INTER_LINEAR if min(roi.shape) < img_size else cv2.INTER_AREA\n",
        "        roi = cv2.resize(roi, (img_size, img_size), interpolation = interp_choice) # roi resize\n",
        "\n",
        "    # normalization\n",
        "    # [0,1]\n",
        "    roi = roi.astype(np.float32) / 255\n",
        "\n",
        "    # greyscale to rgb\n",
        "    roi_rgb = np.stack([roi, roi, roi], axis = 0)\n",
        "\n",
        "    # imagenet normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
        "    std  = np.array([0.229, 0.224, 0.225])[:, None, None]\n",
        "    roi_rgb = (roi_rgb - mean) / std\n",
        "\n",
        "    # to tensor\n",
        "    roi_rgb = torch.from_numpy(roi_rgb)\n",
        "\n",
        "    # label to tensor, discrete number\n",
        "    label_tensor = torch.tensor(label, dtype = torch.long)\n",
        "\n",
        "    return roi_rgb, label_tensor"
      ],
      "metadata": {
        "id": "7aM2g0xLfpmL"
      },
      "id": "7aM2g0xLfpmL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grad-CAM configuration"
      ],
      "metadata": {
        "id": "b8UZenWdsBnq"
      },
      "id": "b8UZenWdsBnq"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from pytorch_grad_cam import GradCAM\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "# config the grad-CAM\n",
        "model = models.mobilenet_v2(weights = None)\n",
        "model.classifier[1] = nn.Linear(model.last_channel, 4)\n",
        "\n",
        "# load the fine-tune weights, with a random fold\n",
        "best_weight_path = Root / 'best_weight_finetune'\n",
        "model.load_state_dict(torch.load(best_weight_path / f'fold4_best.pth'))\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# config gradcam\n",
        "# use the last conv2d layer\n",
        "cam = GradCAM(model = model, target_layers = [model.features[-1]])"
      ],
      "metadata": {
        "id": "mfLB8qbqkgR6"
      },
      "id": "mfLB8qbqkgR6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate and save the result"
      ],
      "metadata": {
        "id": "WJYvjccFsFnS"
      },
      "id": "WJYvjccFsFnS"
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# get the visualization\n",
        "grad_cam_path = Root / 'grad_cam'\n",
        "grad_cam_path.mkdir(exist_ok = True)\n",
        "\n",
        "# extract the file from fold 4\n",
        "train_dataloader, val_dataloader = all_dataloader[4]\n",
        "fold4 = train_dataloader.dataset\n",
        "\n",
        "for (mrc_path, svg_path, label) in tqdm(fold4.items[:30], desc = 'grad-cam process'):\n",
        "    # get the ROI and label tensor\n",
        "    tensor, label = roi_process(mrc_path, svg_path, label)\n",
        "\n",
        "    # [3,224,224] to [1,3,224,224]\n",
        "    tensor = tensor[None,:,:,:].float()\n",
        "\n",
        "    # construct the target class (1,2,3,4) list\n",
        "    targets = [ClassifierOutputTarget(label)]\n",
        "\n",
        "    # get first heatmap, because batch_size = 1\n",
        "    outcome = cam(input_tensor = tensor, targets = targets)[0]\n",
        "\n",
        "    # tensor to numpy array, drop batch_size\n",
        "    tensor2np = tensor[0].detach().cpu().numpy()\n",
        "\n",
        "    # [3,224,224] to [224,224,3]\n",
        "    tensor2np = tensor2np.transpose(1, 2, 0).astype(np.float32)\n",
        "\n",
        "    # anti-normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406], dtype = np.float32)\n",
        "    std  = np.array([0.229, 0.224, 0.225], dtype = np.float32)\n",
        "    imgtensor2np = tensor2np * std[None, None, :] + mean[None, None, :]\n",
        "\n",
        "    # save final result\n",
        "    cam_img = show_cam_on_image(imgtensor2np, outcome, use_rgb = True)\n",
        "    name = Path(mrc_path).stem\n",
        "    save_path = grad_cam_path / f\"{name}_camresult.png\"\n",
        "    plt.imsave(str(save_path), cam_img)"
      ],
      "metadata": {
        "id": "OuePGo2CmBYN"
      },
      "id": "OuePGo2CmBYN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}